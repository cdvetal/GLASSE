{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTNWPV0zcx5R"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3rwoedyJ_aJ",
        "outputId": "0df05bb7-6f16-4475-ac1f-b694ffc1d118"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting deap\n",
            "  Downloading deap-1.3.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 KB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from deap) (1.22.4)\n",
            "Installing collected packages: deap\n",
            "Successfully installed deap-1.3.3\n"
          ]
        }
      ],
      "source": [
        "!pip install deap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "joO_auyqJ-9d"
      },
      "outputs": [],
      "source": [
        "from deap import creator, base, tools, algorithms\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow.keras.backend as k\n",
        "import statistics\n",
        "import csv\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APe_JkCaT3nt"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NB_6Ct3FT0wi"
      },
      "outputs": [],
      "source": [
        "def fitness(individual):\n",
        "    return individual.fitness.value,\n",
        "\n",
        "def discriminator_loss(fake_output):\n",
        "  fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "  return fake_loss\n",
        "\n",
        "def evaluateInd(individual):\n",
        "  vector = np.array([individual])\n",
        "  noise = k.constant(vector)\n",
        "  label = tf.keras.utils.to_categorical([number], num_classes)\n",
        "  label = tf.cast(label, tf.float32)\n",
        "  noise_and_labels = tf.concat([noise, label], 1)\n",
        "  fake_image = cond_gan.generator.predict(noise_and_labels,  verbose=False)\n",
        "  fake_image_and_labels = tf.concat([fake_image, image_one_hot_labels], -1)\n",
        "  fake_output = cond_gan.discriminator.predict(fake_image_and_labels,  verbose=False)\n",
        "  total_loss = discriminator_loss(fake_output)\n",
        "  value = 1 - abs(total_loss.numpy() - TARGET),\n",
        "  losses.append((value, total_loss.numpy()))\n",
        "  return value\n",
        "\n",
        "def make_images_from_generation(current_gen, pop, pop_numbers):\n",
        "  # makes images (individuals and activations) for entire generation\n",
        "  \n",
        "  predicted_number = -1\n",
        "  activation = -1\n",
        "\n",
        "  for ind in pop_numbers:\n",
        "    vector = np.array([pop[ind]])\n",
        "    noise = k.constant(vector)\n",
        "    noise_and_labels = tf.concat([noise, label], 1)\n",
        "    \n",
        "    # ind image\n",
        "    test_image = cond_gan.generator.predict(noise_and_labels,  verbose=False)\n",
        "    plt.figure()\n",
        "    plt.imshow(test_image[0, :, :, 0], cmap='gray')\n",
        "    # uncomment to save individual image\n",
        "    # plt.savefig(f\"{path_images_generation}/gen_{current_gen}_ind_{ind}\")\n",
        "    plt.close()\n",
        "\n",
        "    prediction = classif.predict(test_image,  verbose=False)\n",
        "    \n",
        "    # uncomment to save classifier activations image\n",
        "    # plt.figure()\n",
        "    # plt.bar(v, prediction[0], color ='maroon',width = 0.4)\n",
        "    # plt.ylim(0, 1)\n",
        "    # plt.plot(v, [0.5]*10, '--k')\n",
        "    # plt.savefig(f\"{path_images_classifier}/gen_{current_gen}_ind_{ind}\")\n",
        "    # plt.close()\n",
        "\n",
        "    if ind == 0:\n",
        "      predicted_number = np.argmax(prediction)\n",
        "      activation = np.max(prediction)\n",
        "\n",
        "    list_predictions[ind] = np.argmax(prediction)\n",
        "    list_activations[ind] = np.max(prediction)\n",
        "\n",
        "    if list_predictions[ind] != number and list_activations[ind] >= 0.5:\n",
        "      adv_example = [number, list_predictions[ind], list_activations[ind], current_gen, ind, losses[ind][1], pop[ind]]\n",
        "      writer_adv.writerow(adv_example)\n",
        "  \n",
        "  return (predicted_number, activation)\n",
        "\n",
        "def gen_graphic(current_gen, smallest, biggest, mean):\n",
        "\n",
        "  list_gen = list(range(0, current_gen+1))\n",
        "  sm = np.reshape(smallest, (current_gen+1, ))\n",
        "  bg = np.reshape(biggest, (current_gen+1, ))\n",
        "  me = np.reshape(mean, (current_gen+1, ))\n",
        "\n",
        "  plt.figure()\n",
        "  if current_gen == 0:\n",
        "    plt.plot(list_gen, sm, '-x', label=\"Smallests\", color=\"red\")\n",
        "    plt.plot(list_gen, bg, '-x', label=\"Biggests\", color=\"green\")\n",
        "    plt.plot(list_gen, me,  '-x', label=\"Average\", color=\"yellow\")\n",
        "  else:\n",
        "    plt.plot(list_gen, sm, label=\"Smallests\", color=\"red\")\n",
        "    plt.plot(list_gen, bg, label=\"Biggests\", color=\"green\")\n",
        "    plt.plot(list_gen, me, label=\"Average\", color=\"yellow\")\n",
        "      \n",
        "\n",
        "  plt.xlabel(\"Generation\")\n",
        "  plt.ylabel(\"Fitness\")\n",
        "  plt.xlim(0, generations)\n",
        "  plt.legend([\"Smallest fitness value in generation\", \"Biggest fitness value in generation\", \"Mean fitness value in generation\"])\n",
        "  plt.savefig(f'{path_graphic}/grafico_{current_gen}.png')\n",
        "  plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chI7096gUB6i"
      },
      "outputs": [],
      "source": [
        "seed = 0\n",
        "path_weights = './weights'\n",
        "num_classes = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mJh2_m3UEX-"
      },
      "source": [
        "## Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOYzl2ErUFIA"
      },
      "outputs": [],
      "source": [
        "# import classifier\n",
        "classif = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (5,5), padding='same', activation='relu', input_shape=(28, 28,1)),\n",
        "    tf.keras.layers.Conv2D(32, (5,5), padding='same', activation='relu'),\n",
        "    tf.keras.layers.MaxPool2D(),\n",
        "    tf.keras.layers.Dropout(0.25),\n",
        "    tf.keras.layers.Conv2D(64, (3,3), padding='same', activation='relu'),\n",
        "    tf.keras.layers.Conv2D(64, (3,3), padding='same', activation='relu'),\n",
        "    tf.keras.layers.MaxPool2D(strides=(2,2)),\n",
        "    tf.keras.layers.Dropout(0.25),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "classif.load_weights(f'{path_weights}/classifier_weights.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeOqb7-7XUwK"
      },
      "source": [
        "## Conditional GAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqsEKEHKXdlH"
      },
      "outputs": [],
      "source": [
        "# Conditional GAN\n",
        "batch_size = 64\n",
        "num_channels = 1\n",
        "num_classes = 10\n",
        "image_size = 28\n",
        "latent_dim = 128\n",
        "\n",
        "generator_in_channels = latent_dim + num_classes\n",
        "discriminator_in_channels = num_channels + num_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Skrth6bSXilI"
      },
      "outputs": [],
      "source": [
        "# Create the discriminator.\n",
        "discriminator = tf.keras.Sequential(\n",
        "    [\n",
        "        tf.keras.layers.InputLayer((28, 28, discriminator_in_channels)),\n",
        "        layers.Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.GlobalMaxPooling2D(),\n",
        "        layers.Dense(1),\n",
        "    ],\n",
        "    name=\"discriminator\",\n",
        ")\n",
        "\n",
        "# Create the generator.\n",
        "generator = tf.keras.Sequential(\n",
        "    [\n",
        "        tf.keras.layers.InputLayer((generator_in_channels,)),\n",
        "        # We want to generate 128 + num_classes coefficients to reshape into a\n",
        "        # 7x7x(128 + num_classes) map.\n",
        "        layers.Dense(7 * 7 * generator_in_channels),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Reshape((7, 7, generator_in_channels)),\n",
        "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(1, (7, 7), padding=\"same\", activation=\"sigmoid\"),\n",
        "    ],\n",
        "    name=\"generator\",\n",
        ")\n",
        "\n",
        "class ConditionalGAN(tf.keras.Model):\n",
        "    def __init__(self, discriminator, generator, latent_dim):\n",
        "        super(ConditionalGAN, self).__init__()\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.latent_dim = latent_dim\n",
        "        self.gen_loss_tracker = tf.keras.metrics.Mean(name=\"generator_loss\")\n",
        "        self.disc_loss_tracker = tf.keras.metrics.Mean(name=\"discriminator_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.gen_loss_tracker, self.disc_loss_tracker]\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
        "        super(ConditionalGAN, self).compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "\n",
        "    def train_step(self, data):\n",
        "        # Unpack the data.\n",
        "        real_images, one_hot_labels = data\n",
        "\n",
        "        # Add dummy dimensions to the labels so that they can be concatenated with\n",
        "        # the images. This is for the discriminator.\n",
        "        image_one_hot_labels = one_hot_labels[:, :, None, None]\n",
        "        image_one_hot_labels = tf.repeat(\n",
        "            image_one_hot_labels, repeats=[image_size * image_size]\n",
        "        )\n",
        "        image_one_hot_labels = tf.reshape(\n",
        "            image_one_hot_labels, (-1, image_size, image_size, num_classes)\n",
        "        )\n",
        "\n",
        "        # Sample random points in the latent space and concatenate the labels.\n",
        "        # This is for the generator.\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "        random_vector_labels = tf.concat(\n",
        "            [random_latent_vectors, one_hot_labels], axis=1\n",
        "        )\n",
        "\n",
        "        # Decode the noise (guided by labels) to fake images.\n",
        "        generated_images = self.generator(random_vector_labels)\n",
        "\n",
        "        # Combine them with real images. Note that we are concatenating the labels\n",
        "        # with these images here.\n",
        "        fake_image_and_labels = tf.concat([generated_images, image_one_hot_labels], -1)\n",
        "        real_image_and_labels = tf.concat([real_images, image_one_hot_labels], -1)\n",
        "        combined_images = tf.concat(\n",
        "            [fake_image_and_labels, real_image_and_labels], axis=0\n",
        "        )\n",
        "\n",
        "        # Assemble labels discriminating real from fake images.\n",
        "        labels = tf.concat(\n",
        "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
        "        )\n",
        "\n",
        "        # Train the discriminator.\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(combined_images)\n",
        "            d_loss = self.loss_fn(labels, predictions)\n",
        "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
        "        self.d_optimizer.apply_gradients(\n",
        "            zip(grads, self.discriminator.trainable_weights)\n",
        "        )\n",
        "\n",
        "        # Sample random points in the latent space.\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "        random_vector_labels = tf.concat(\n",
        "            [random_latent_vectors, one_hot_labels], axis=1\n",
        "        )\n",
        "\n",
        "        # Assemble labels that say \"all real images\".\n",
        "        misleading_labels = tf.zeros((batch_size, 1))\n",
        "\n",
        "        # Train the generator (note that we should *not* update the weights\n",
        "        # of the discriminator)!\n",
        "        with tf.GradientTape() as tape:\n",
        "            fake_images = self.generator(random_vector_labels)\n",
        "            fake_image_and_labels = tf.concat([fake_images, image_one_hot_labels], -1)\n",
        "            predictions = self.discriminator(fake_image_and_labels)\n",
        "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
        "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
        "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
        "\n",
        "        # Monitor loss.\n",
        "        self.gen_loss_tracker.update_state(g_loss)\n",
        "        self.disc_loss_tracker.update_state(d_loss)\n",
        "        return {\n",
        "            \"g_loss\": self.gen_loss_tracker.result(),\n",
        "            \"d_loss\": self.disc_loss_tracker.result(),\n",
        "        }\n",
        "\n",
        "cond_gan = ConditionalGAN(\n",
        "    discriminator=discriminator, generator=generator, latent_dim=latent_dim\n",
        ")\n",
        "\n",
        "cond_gan.generator.load_weights(f'{path_weights}/cond_gan_gen_weights.h5')\n",
        "cond_gan.discriminator.load_weights(f'{path_weights}/cond_gan_dis_weights.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKXKZFeQXkMv"
      },
      "source": [
        "## GA "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wadzoPaXn0T"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "\n",
        "npop = 100\n",
        "n = latent_dim\n",
        "TARGET = 0.5\n",
        "CXPB = 1\n",
        "MUTPB = 1\n",
        "generations = 40\n",
        "mu = 0\n",
        "sigma = 3\n",
        "indpb = 0.1\n",
        "tournsize = 3\n",
        "ELITE_SIZE = 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_uOWKJdX5CF"
      },
      "outputs": [],
      "source": [
        "# Parent Directory path\n",
        "new_folder = \"./output/\"\n",
        "mode = 0o777\n",
        "os.mkdir(new_folder, mode)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9ObyoKEXr2W"
      },
      "source": [
        "### GA Cycle: numbers from 0 to 9, runs from 1 to 15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uaLTNlICJ59r"
      },
      "outputs": [],
      "source": [
        "for number in range(0,10):\n",
        "  stat_folder_digit = os.path.join(new_folder, f\"stat_digit_{number}\")\n",
        "  os.mkdir(stat_folder_digit, mode)\n",
        "  for run in range(1,16):\n",
        "    start = time.time()\n",
        "\n",
        "    print(run)\n",
        "\n",
        "    time_file = f\"{new_folder}/time.csv\"\n",
        "    f_time = open(time_file, 'a')\n",
        "    writer_time = csv.writer(f_time)\n",
        "\n",
        "    results_folder = os.path.join(new_folder, f\"results_digit_{number}_run_{run}\")\n",
        "    os.mkdir(results_folder, mode)\n",
        "    images_folder = os.path.join(results_folder, \"images\")\n",
        "    os.mkdir(images_folder, mode)\n",
        "    path_images_classifier = os.path.join(images_folder, \"classifier\")\n",
        "    os.mkdir(path_images_classifier, mode)\n",
        "    path_images_generation = os.path.join(images_folder, \"generation\")\n",
        "    os.mkdir(path_images_generation, mode)\n",
        "    path_vectors = os.path.join(results_folder, \"vectors\")\n",
        "    os.mkdir(path_vectors, mode)\n",
        "    path_graphic = os.path.join(results_folder, \"graphics\")\n",
        "    os.mkdir(path_graphic, mode)\n",
        "    path_summary = os.path.join(results_folder, \"summary\")\n",
        "    os.mkdir(path_summary, mode)\n",
        "\n",
        "    path_statinfo = os.path.join(stat_folder_digit, f\"Arquivo_digit_{number}_run_{run}.csv\")\n",
        "    path_fit = os.path.join(stat_folder_digit, f\"Fit_digit_{number}_run_{run}.csv\")\n",
        "\n",
        "    # save parameters\n",
        "    path_param = f\"{path_summary}/param.csv\"\n",
        "    f_param = open(path_param, 'w')\n",
        "    writer_param = csv.writer(f_param)\n",
        "    header_param = ['number', 'latent_dim', 'npop', 'target', 'elite size', 'ngen','cxpb', 'mutpb','mu','sigma', 'indpb','tournsize']\n",
        "    writer_param.writerow(header_param)\n",
        "    param_list = [number, latent_dim, npop, TARGET, ELITE_SIZE, generations, CXPB, MUTPB, mu, sigma, indpb, tournsize]\n",
        "    writer_param.writerow(param_list)\n",
        "    f_param.close()\n",
        "\n",
        "    # Add dummy dimensions to the labels so that they can be concatenated with\n",
        "    # the images. This is for the discriminator.\n",
        "    label = tf.keras.utils.to_categorical([number], num_classes)\n",
        "    image_one_hot_labels = label[:, :, None, None]\n",
        "    image_one_hot_labels = tf.repeat(image_one_hot_labels, repeats=[image_size * image_size])\n",
        "    image_one_hot_labels = tf.reshape(image_one_hot_labels, (-1, image_size, image_size, num_classes))\n",
        "\n",
        "    losses = []\n",
        "    count_inc_pred = [0]*generations\n",
        "    count_adv = [0]*generations\n",
        "    count_in_interval = [0]*generations\n",
        "\n",
        "    # Initialization:\n",
        "    creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
        "    creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
        "\n",
        "    toolbox = base.Toolbox()\n",
        "\n",
        "    toolbox.register(\"attr_flt\", np.random.normal)\n",
        "    toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_flt, n)\n",
        "    toolbox.register(\"Population\", tools.initRepeat, list, toolbox.individual)\n",
        "    population = toolbox.Population(n=npop)\n",
        "\n",
        "    cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "    toolbox.register(\"mate\", tools.cxTwoPoint)\n",
        "    toolbox.register(\"mutate\", tools.mutGaussian, mu=mu, sigma=sigma, indpb=indpb)\n",
        "    toolbox.register(\"select\", tools.selTournament, tournsize=tournsize, fit_attr='fitness') \n",
        "    toolbox.register(\"evaluate\", evaluateInd)\n",
        "\n",
        "    smallest = []\n",
        "    biggest = []\n",
        "    mean = []\n",
        "    v = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "    header = ['ind_number', 'fitness value', 'abs(fake_loss-target)', 'fake_loss','number', 'predicted number', 'activation',  'vector']\n",
        "\n",
        "    path_all = f\"{path_summary}/summary.csv\"\n",
        "    f_all = open(path_all, 'w')\n",
        "    writer_all = csv.writer(f_all)\n",
        "\n",
        "    header_all = ['generation', 'fitness smallest', 'fitness biggest', 'fitness mean', 'fitness std', 'fake_loss worst', 'fake_loss best', 'fake_loss mean', 'fake_loss std', 'right number', 'choosen number', 'activation', 'best vec', 'centroid vec']\n",
        "    writer_all.writerow(header_all)\n",
        "\n",
        "    path_adv = f\"{path_summary}/adv.csv\"\n",
        "    f_adv = open(path_adv, 'w')\n",
        "    writer_adv = csv.writer(f_adv)\n",
        "\n",
        "    header_adv = [\"number\", \"predicted number\", \"activation\", \"generation\",\"ind\", \"fake_loss\", \"vector\"]\n",
        "    writer_adv.writerow(header_adv)\n",
        "\n",
        "    f_stat = open(path_statinfo, 'w')\n",
        "    writer_stat = csv.writer(f_stat)\n",
        "    header_stat = ['number', 'run'\t,'geração',\t'numero de classificações erradas',\t'numero de disc loss no intervalo',\t'numero de adversariais']\n",
        "    writer_stat.writerow(header_stat)\n",
        "\n",
        "    f_fit = open(path_fit, 'w')\n",
        "    writer_fit = csv.writer(f_fit)\n",
        "    writer_fit.writerow(['fitness smallest', 'fitness biggest', 'fitness mean', 'fitness std'])\n",
        "\n",
        "    for g in range(0, generations):\n",
        "\n",
        "      list_predictions = ['']*100\n",
        "      list_activations = ['']*100\n",
        "      path = f\"{path_vectors}/gen{g}.csv\"\n",
        "      f = open(path, 'w')\n",
        "      writer = csv.writer(f)\n",
        "      writer.writerow(header)\n",
        "\n",
        "      if g > 0:\n",
        "        losses = [losses[0]]\n",
        "        # Select and clone the next generation individuals\n",
        "        offspring = toolbox.select(population, len(population) - ELITE_SIZE)\n",
        "        # offspring = map(toolbox.clone, offspring)\n",
        "\n",
        "        # Aplly mutation and crossover on the offspring\n",
        "        offspring = algorithms.varAnd(offspring, toolbox, CXPB, MUTPB)\n",
        "\n",
        "        # Evaluate the individuals with an invalid fitness\n",
        "        fitnesses = toolbox.map(toolbox.evaluate, offspring)\n",
        "        for ind, fit in zip(offspring, fitnesses):\n",
        "          ind.fitness.values = fit\n",
        "        \n",
        "        # Select elite from population, rest from offspring\n",
        "        population.sort(key=lambda x: x.fitness, reverse=True)\n",
        "        population = population[:ELITE_SIZE] + offspring\n",
        "        population.sort(key=lambda x: x.fitness, reverse=True)\n",
        "\n",
        "      else:\n",
        "        fitnesses = toolbox.map(toolbox.evaluate, population)\n",
        "        for ind, fit in zip(population, fitnesses):\n",
        "          ind.fitness.values = fit\n",
        "        population.sort(key=lambda x: x.fitness, reverse=True)\n",
        "\n",
        "      losses.sort(key=lambda x: x[0], reverse=True)\n",
        "      pop_numbers = list(range(npop))  \n",
        "      (predicted_number, activation) = make_images_from_generation(g, population, pop_numbers)\n",
        "      \n",
        "      for ind in range(npop):\n",
        "        info = [ind, population[ind].fitness.values[0], 1 - population[ind].fitness.values[0], losses[ind][1], number, list_predictions[ind], list_activations[ind], population[ind]]\n",
        "        writer.writerow(info)\n",
        "\n",
        "      bigInd = population[0].fitness.values\n",
        "      biggest.append(bigInd)\n",
        "      smallInd = population[npop-1].fitness.values\n",
        "      smallest.append(smallInd)\n",
        "      print(f\"\\nGen {g}\")\n",
        "      print(f\"Fitness value smallest: {smallInd} \\tFitness value biggest {bigInd}\")\n",
        "      m = sum(ind.fitness.values[0] for ind in population)/npop\n",
        "      mean.append(m)\n",
        "\n",
        "      writer.writerow([\"\"])\n",
        "      writer.writerow(['fitness smallest', 'fitness biggest', 'fitness mean', 'fitness std'])\n",
        "      row_fitness = [smallInd[0], bigInd[0], m, np.std(list(ind.fitness.values[0] for ind in population))]\n",
        "      writer.writerow(row_fitness)\n",
        "\n",
        "      writer.writerow(['fake_loss worst', 'fake_loss best', 'fake_loss mean', 'fake_loss std'])\n",
        "      row_fake_loss = [losses[npop-1][1], losses[0][1], sum(x[1] for x in losses)/npop, np.std(list(x[1] for x in losses))]\n",
        "      writer.writerow(row_fake_loss)\n",
        "\n",
        "      row_all = [g]\n",
        "      row_all.extend(row_fitness)\n",
        "      row_all.extend(row_fake_loss)\n",
        "      row_all.extend([number, predicted_number, activation])\n",
        "      row_all.append(population[0])\n",
        "      centroid = [sum(sub_list) / len(sub_list) for sub_list in zip(*population)]\n",
        "      row_all.append(centroid)\n",
        "      writer_all.writerow(row_all)\n",
        "\n",
        "      f.close()\n",
        "\n",
        "      gen_graphic(g, smallest, biggest, mean)\n",
        "\n",
        "      for i in range(npop):\n",
        "        if list_predictions[i] != number and list_activations[i] >= 0.5:\n",
        "          count_inc_pred[g] = count_inc_pred[g] + 1\n",
        "          if (1 - population[i].fitness.values[0]) < 0.01:\n",
        "            count_adv[g] = count_adv[g] + 1\n",
        "        if (1 - population[i].fitness.values[0]) < 0.01:\n",
        "            count_in_interval[g] = count_in_interval[g] + 1\n",
        "\n",
        "      info = [number, run, g, count_inc_pred[g], count_in_interval[g], count_adv[g]]\n",
        "      writer_stat.writerow(info)\n",
        "\n",
        "      writer_fit.writerow(row_fitness)\n",
        "\n",
        "    f_all.close()\n",
        "    f_adv.close()\n",
        "    f_stat.close()\n",
        "    f_fit.close()\n",
        "\n",
        "    end = time.time()\n",
        "    elapsed_time = end - start\n",
        "    row_time = [elapsed_time]\n",
        "    writer_time.writerow(row_time)\n",
        "    f_time.close()\n",
        "\n",
        "    seed = seed + 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHVeRJ_8YbGf"
      },
      "source": [
        "### One run: (easier to break like this)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "number = 0\n",
        "run = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNNPvMhGaBLS"
      },
      "outputs": [],
      "source": [
        "# Record time\n",
        "start = time.time()\n",
        "\n",
        "time_file = f\"{new_folder}/time.csv\"\n",
        "f_time = open(time_file, 'a')\n",
        "writer_time = csv.writer(f_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stat_folder_digit = os.path.join(new_folder, f\"stat_digit_{number}\")\n",
        "os.mkdir(stat_folder_digit, mode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kl7vovowaE22"
      },
      "outputs": [],
      "source": [
        "# Folder structure \n",
        "results_folder = os.path.join(new_folder, f\"results_digit_{number}_run_{run}\")\n",
        "os.mkdir(results_folder, mode)\n",
        "images_folder = os.path.join(results_folder, \"images\")\n",
        "os.mkdir(images_folder, mode)\n",
        "path_images_classifier = os.path.join(images_folder, \"classifier\")\n",
        "os.mkdir(path_images_classifier, mode)\n",
        "path_images_generation = os.path.join(images_folder, \"generation\")\n",
        "os.mkdir(path_images_generation, mode)\n",
        "path_vectors = os.path.join(results_folder, \"vectors\")\n",
        "os.mkdir(path_vectors, mode)\n",
        "path_graphic = os.path.join(results_folder, \"graphics\")\n",
        "os.mkdir(path_graphic, mode)\n",
        "path_summary = os.path.join(results_folder, \"summary\")\n",
        "os.mkdir(path_summary, mode)\n",
        "\n",
        "path_statinfo = os.path.join(stat_folder_digit, f\"Arquivo_digit_{number}_run_{run}.csv\")\n",
        "path_fit = os.path.join(stat_folder_digit, f\"Fit_digit_{number}_run_{run}.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OaGs2Qb_aJps"
      },
      "outputs": [],
      "source": [
        "# save parameters\n",
        "path_param = f\"{path_summary}/param.csv\"\n",
        "f_param = open(path_param, 'w')\n",
        "writer_param = csv.writer(f_param)\n",
        "header_param = ['number', 'latent_dim', 'npop', 'target', 'elite size', 'ngen','cxpb', 'mutpb','mu','sigma', 'indpb','tournsize']\n",
        "writer_param.writerow(header_param)\n",
        "param_list = [number, latent_dim, npop, TARGET, ELITE_SIZE, generations, CXPB, MUTPB, mu, sigma, indpb, tournsize]\n",
        "writer_param.writerow(param_list)\n",
        "f_param.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDWESv_RaLWd"
      },
      "outputs": [],
      "source": [
        "# Add dummy dimensions to the labels so that they can be concatenated with\n",
        "# the images. This is for the discriminator.\n",
        "label = tf.keras.utils.to_categorical([number], num_classes)\n",
        "image_one_hot_labels = label[:, :, None, None]\n",
        "image_one_hot_labels = tf.repeat(image_one_hot_labels, repeats=[image_size * image_size])\n",
        "image_one_hot_labels = tf.reshape(image_one_hot_labels, (-1, image_size, image_size, num_classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAabOq-GaN6K"
      },
      "outputs": [],
      "source": [
        "# Prepare to save info about losses, wrong classifications, individuals in interval and number of adversarials\n",
        "losses = []\n",
        "count_inc_pred = [0]*generations\n",
        "count_adv = [0]*generations\n",
        "count_in_interval = [0]*generations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bnXF0nnaY4x"
      },
      "outputs": [],
      "source": [
        "# Initialization:\n",
        "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
        "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
        "\n",
        "toolbox = base.Toolbox()\n",
        "\n",
        "toolbox.register(\"attr_flt\", np.random.normal)\n",
        "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_flt, n)\n",
        "toolbox.register(\"Population\", tools.initRepeat, list, toolbox.individual)\n",
        "population = toolbox.Population(n=npop)\n",
        "\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
        "toolbox.register(\"mutate\", tools.mutGaussian, mu=mu, sigma=sigma, indpb=indpb)\n",
        "toolbox.register(\"select\", tools.selTournament, tournsize=tournsize, fit_attr='fitness') \n",
        "toolbox.register(\"evaluate\", evaluateInd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mFc7byDab7H"
      },
      "outputs": [],
      "source": [
        "# Prepare .csv files \n",
        "\n",
        "# record smallest, biggest and fitness values in a generation\n",
        "smallest = []\n",
        "biggest = []\n",
        "mean = []\n",
        "v = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "\n",
        "# this header is for the generation file which saves the vectors from a generation and some important information about them \n",
        "header = ['ind_number', 'fitness value', 'abs(fake_loss-target)', 'fake_loss','number', 'predicted number', 'activation',  'vector']\n",
        "\n",
        "# this file is a summary of the process\n",
        "path_all = f\"{path_summary}/summary.csv\"\n",
        "f_all = open(path_all, 'w')\n",
        "writer_all = csv.writer(f_all)\n",
        "\n",
        "header_all = ['generation', 'fitness smallest', 'fitness biggest', 'fitness mean', 'fitness std', 'fake_loss worst', 'fake_loss best', 'fake_loss mean', 'fake_loss std', 'right number', 'choosen number', 'activation', 'best vec', 'centroid vec']\n",
        "writer_all.writerow(header_all)\n",
        "\n",
        "# this file saves misclassifications\n",
        "path_adv = f\"{path_summary}/adv.csv\"\n",
        "f_adv = open(path_adv, 'w')\n",
        "writer_adv = csv.writer(f_adv)\n",
        "\n",
        "header_adv = [\"number\", \"predicted number\", \"activation\", \"generation\",\"ind\", \"fake_loss\", \"vector\"]\n",
        "writer_adv.writerow(header_adv)\n",
        "\n",
        "# this file saves statistic information (number, run, generation, number of misclassifications, number of individuals with disc loss in the interval, and number of adversarials - in interval and misclassified)\n",
        "f_stat = open(path_statinfo, 'w')\n",
        "writer_stat = csv.writer(f_stat)\n",
        "header_stat = ['number', 'run'\t,'geração',\t'numero de classificações erradas',\t'numero de disc loss no intervalo',\t'numero de adversariais']\n",
        "writer_stat.writerow(header_stat)\n",
        "\n",
        "# this file saves statistic info about the fitness, it is repetitive as this info is also on each generation file (referred to in 'header'), but it is useful to keep everything in one file\n",
        "f_fit = open(path_fit, 'w')\n",
        "writer_fit = csv.writer(f_fit)\n",
        "writer_fit.writerow(['fitness smallest', 'fitness biggest', 'fitness mean', 'fitness std'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhtesbb0YfZO"
      },
      "outputs": [],
      "source": [
        "# Generations\n",
        "for g in range(0, generations):\n",
        "  list_predictions = ['']*100\n",
        "  list_activations = ['']*100\n",
        "\n",
        "  # this is the generation file\n",
        "  path = f\"{path_vectors}/gen{g}.csv\"\n",
        "  f = open(path, 'w')\n",
        "  writer = csv.writer(f)\n",
        "  writer.writerow(header)\n",
        "\n",
        "  if g > 0:\n",
        "    losses = [losses[0]]\n",
        "    # Select and clone the next generation individuals\n",
        "    offspring = toolbox.select(population, len(population) - ELITE_SIZE)\n",
        "    # offspring = map(toolbox.clone, offspring)\n",
        "\n",
        "    # Aplly mutation and crossover on the offspring\n",
        "    offspring = algorithms.varAnd(offspring, toolbox, CXPB, MUTPB)\n",
        "\n",
        "    # Evaluate the individuals with an invalid fitness\n",
        "    fitnesses = toolbox.map(toolbox.evaluate, offspring)\n",
        "    for ind, fit in zip(offspring, fitnesses):\n",
        "      ind.fitness.values = fit\n",
        "        \n",
        "    # Select elite from population, rest from offspring\n",
        "    population.sort(key=lambda x: x.fitness, reverse=True)\n",
        "    population = population[:ELITE_SIZE] + offspring\n",
        "    population.sort(key=lambda x: x.fitness, reverse=True)\n",
        "\n",
        "  else:\n",
        "    fitnesses = toolbox.map(toolbox.evaluate, population)\n",
        "    for ind, fit in zip(population, fitnesses):\n",
        "        ind.fitness.values = fit\n",
        "    population.sort(key=lambda x: x.fitness, reverse=True)\n",
        "\n",
        "  losses.sort(key=lambda x: x[0], reverse=True)\n",
        "  pop_numbers = list(range(npop))  \n",
        "  (predicted_number, activation) = make_images_from_generation(g, population, pop_numbers)\n",
        "      \n",
        "  for ind in range(npop):\n",
        "    info = [ind, population[ind].fitness.values[0], 1 - population[ind].fitness.values[0], losses[ind][1], number, list_predictions[ind], list_activations[ind], population[ind]]\n",
        "    writer.writerow(info)\n",
        "\n",
        "  bigInd = population[0].fitness.values\n",
        "  biggest.append(bigInd)\n",
        "  smallInd = population[npop-1].fitness.values\n",
        "  smallest.append(smallInd)\n",
        "  print(f\"\\nGen {g}\")\n",
        "  print(f\"Fitness value smallest: {smallInd} \\tFitness value biggest {bigInd}\")\n",
        "  m = sum(ind.fitness.values[0] for ind in population)/npop\n",
        "  mean.append(m)\n",
        "\n",
        "  writer.writerow([\"\"])\n",
        "  writer.writerow(['fitness smallest', 'fitness biggest', 'fitness mean', 'fitness std'])\n",
        "  row_fitness = [smallInd[0], bigInd[0], m, np.std(list(ind.fitness.values[0] for ind in population))]\n",
        "  writer.writerow(row_fitness)\n",
        "\n",
        "  writer.writerow(['fake_loss worst', 'fake_loss best', 'fake_loss mean', 'fake_loss std'])\n",
        "  row_fake_loss = [losses[npop-1][1], losses[0][1], sum(x[1] for x in losses)/npop, np.std(list(x[1] for x in losses))]\n",
        "  writer.writerow(row_fake_loss)\n",
        "\n",
        "  row_all = [g]\n",
        "  row_all.extend(row_fitness)\n",
        "  row_all.extend(row_fake_loss)\n",
        "  row_all.extend([number, predicted_number, activation])\n",
        "  row_all.append(population[0])\n",
        "  centroid = [sum(sub_list) / len(sub_list) for sub_list in zip(*population)]\n",
        "  row_all.append(centroid)\n",
        "  writer_all.writerow(row_all)\n",
        "\n",
        "  f.close()\n",
        "\n",
        "  gen_graphic(g, smallest, biggest, mean)\n",
        "\n",
        "  for i in range(npop):\n",
        "    if list_predictions[i] != number and list_activations[i] >= 0.5:\n",
        "      count_inc_pred[g] = count_inc_pred[g] + 1\n",
        "      if (1 - population[i].fitness.values[0]) < 0.01:\n",
        "        count_adv[g] = count_adv[g] + 1\n",
        "    if (1 - population[i].fitness.values[0]) < 0.01:\n",
        "      count_in_interval[g] = count_in_interval[g] + 1\n",
        "\n",
        "  info = [number, run, g, count_inc_pred[g], count_in_interval[g], count_adv[g]]\n",
        "  writer_stat.writerow(info)\n",
        "\n",
        "  writer_fit.writerow(row_fitness)\n",
        "\n",
        "f_all.close()\n",
        "f_adv.close()\n",
        "f_stat.close()\n",
        "f_fit.close()\n",
        "\n",
        "end = time.time()\n",
        "elapsed_time = end - start\n",
        "row_time = [elapsed_time]\n",
        "writer_time.writerow(row_time)\n",
        "f_time.close()\n",
        "seed = seed + 1\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
